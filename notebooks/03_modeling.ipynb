{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90de5850",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecbd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 03_modeling.ipynb\n",
    "# Modeling and Evaluation for Diabetes Binary Classification\n",
    "# Extended with Hyperparameter Tuning, Multiple Models, ANN, and Cost-Benefit Analysis\n",
    "# Integrated with preprocessing module\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Notes:\n",
    "- RandomizedSearchCV is used for efficiency.\n",
    "- For expensive models (Random Forest, XGBoost) tuning runs on a 30% subsample, then the best estimator is refit on the full training set.\n",
    "- CV for RandomizedSearchCV is set to cv=5 as requested.\n",
    "\"\"\"\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from pyexpat import features\n",
    "\n",
    "# Add src directory to Python path to import preprocessing module\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import preprocessing module\n",
    "try:\n",
    "    from dpp.preprocessing import get_preprocessed_data\n",
    "    PREPROCESSING_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    PREPROCESSING_AVAILABLE = False\n",
    "    print(f\"Warning: Could not import preprocessing module. Error: {e}\")\n",
    "    print(\"Using placeholder data for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1eb0c6",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0387f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD PREPROCESSED DATA\n",
      "================================================================================\n",
      "[preprocessing] Loading data from: C:\\Users\\Eyyub\\Desktop\\StackFuel\\PortfolioProjekt\\DPP-Stackfuel-Data-Science-Projekt\\data\\raw\\diabetes-health-indicators-dataset\\diabetes_binary_health_indicators_BRFSS2015.csv\n",
      "Training set size: 311,002\n",
      "Test set size: 45,895\n",
      "Number of features: 24\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD PREPROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOAD PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if PREPROCESSING_AVAILABLE:\n",
    "    # Load preprocessed data from preprocessing module\n",
    "    try:\n",
    "        data = get_preprocessed_data()\n",
    "        features_train = data['features_train']\n",
    "        target_train = data['target_train']\n",
    "        features_test = data['features_test']\n",
    "        target_test = data['target_test']\n",
    "\n",
    "        print(f\"Training set size: {len(features_train):,}\")\n",
    "        print(f\"Test set size: {len(features_test):,}\")\n",
    "        print(f\"Number of features: {features_train.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Using placeholder data for demonstration.\")\n",
    "        PREPROCESSING_AVAILABLE = False\n",
    "else:\n",
    "    print(\"Preprocessing module not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198bf6c",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7070d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL INITIALIZATION (BASE ESTIMATORS)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. MODEL INITIALIZATION (base estimators)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL INITIALIZATION (BASE ESTIMATORS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "base_estimators = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    # Use LinearSVC as base; wrap in CalibratedClassifierCV later if you need probabilities\n",
    "    'SVM': CalibratedClassifierCV(LinearSVC(random_state=42, max_iter=10000, dual=False), cv=5),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=500)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7521fc",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eb4f8",
   "metadata": {},
   "source": [
    "In this section, hyperparameter tuning is performed only for four selected models:\n",
    "Random Forest, XGBoost, Logistic Regression, and Neural Network.\n",
    "\n",
    "The rationale behind this selective tuning approach is based on a balance between \n",
    "computational efficiency and expected impact on model performance:\n",
    "\n",
    "1. Effectiveness of Hyperparameters:\n",
    "   These four models have hyperparameters that significantly influence their \n",
    "   predictive performance. For example, the number of trees and depth in Random Forest,\n",
    "   learning rate and tree depth in XGBoost, regularization strength in Logistic Regression,\n",
    "   and architecture parameters in Neural Networks can greatly affect results.\n",
    "\n",
    "2. Computational Cost:\n",
    "   Exhaustive hyperparameter tuning (e.g., GridSearchCV) is computationally expensive,\n",
    "   especially when applied to many models with large parameter grids. Focusing on \n",
    "   models with the highest potential gain optimizes resource usage.\n",
    "\n",
    "3. Simplicity and Speed of Other Models:\n",
    "   Other models like Decision Trees, K-Nearest Neighbors, Naive Bayes, and Linear SVM \n",
    "   typically have fewer or less impactful hyperparameters, or are inherently faster to train.\n",
    "   Their default parameters often provide reasonable baseline performance.\n",
    "\n",
    "4. Practical Workflow:\n",
    "   This approach allows for a manageable and efficient modeling pipeline, prioritizing \n",
    "   tuning efforts where they are most likely to yield substantial improvements.\n",
    "\n",
    "If desired, hyperparameter tuning can be extended to additional models using more \n",
    "efficient search strategies (e.g., RandomizedSearchCV) or by tuning a smaller subset \n",
    "of parameters to balance performance gains with computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "Note: Tuning for expensive models (RF, XGB) runs on a 30% subsample to save time.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. HYPERPARAMETER TUNING (RandomizedSearchCV, CV=5)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"Note: Tuning for expensive models (RF, XGB) runs on a 30% subsample to save time.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a 30% subsample for expensive model tuning\n",
    "features_sub, _, target_sub, _ = train_test_split(features_train, target_train, train_size=0.30, stratify=target_train, random_state=42)\n",
    "\n",
    "# Parameter distributions for RandomizedSearch\n",
    "param_distributions = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [8, 12, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', 0.5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "# We will store the final trained models here\n",
    "trained_models = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf68775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run RandomizedSearchCV and refit best estimator on full training set\n",
    "def tune_and_refit(name, estimator, param_dist, X_tune, y_tune, X_full, y_full, n_iter=20, cv=5):\n",
    "    \"\"\"\n",
    "    Tune estimator with RandomizedSearchCV on X_tune/y_tune (subsample for speed),\n",
    "    then refit the best estimator on the full training data X_full/y_full.\n",
    "    Returns the refitted best estimator and the best CV score.\n",
    "    \"\"\"\n",
    "    print(f\"[tuning] {name}: Starting RandomizedSearchCV (n_iter={n_iter}, cv={cv}) ...\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring='roc_auc',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rs.fit(X_tune, y_tune)\n",
    "    t1 = time.time()\n",
    "    print(f\"[tuning] {name}: RandomizedSearchCV finished in {(t1-t0)/60:.2f} min. Best CV score: {rs.best_score_:.4f}\")\n",
    "    print(f\"[tuning] {name}: Best params: {rs.best_params_}\")\n",
    "\n",
    "    # Refit best estimator on full training data for final model\n",
    "    best_est = rs.best_estimator_\n",
    "    print(f\"[tuning] {name}: Refitting best estimator on the full training set ...\")\n",
    "    t2 = time.time()\n",
    "    best_est.fit(X_full, y_full)\n",
    "    t3 = time.time()\n",
    "    print(f\"[tuning] {name}: Refit finished in {(t3-t2)/60:.2f} min.\")\n",
    "    return best_est, rs.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2434bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Random Forest: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[tuning] Random Forest: RandomizedSearchCV finished in 15.08 min. Best CV score: 0.9598\n",
      "[tuning] Random Forest: Best params: {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': None}\n",
      "[tuning] Random Forest: Refitting best estimator on the full training set ...\n",
      "[tuning] Random Forest: Refit finished in 0.55 min.\n"
     ]
    }
   ],
   "source": [
    "# Tuning loop for selected models\n",
    "tuned_models = {}\n",
    "tuned_scores = {}\n",
    "\n",
    "# Random Forest tuning (on subsample, then refit on full)\n",
    "if 'Random Forest' in base_estimators:\n",
    "    rf_est = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    rf_param_dist = param_distributions['Random Forest']\n",
    "    best_rf, best_rf_score = tune_and_refit('Random Forest', rf_est, rf_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['Random Forest'] = best_rf\n",
    "    tuned_scores['Random Forest'] = best_rf_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94e7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] XGBoost: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[tuning] XGBoost: RandomizedSearchCV finished in 2.36 min. Best CV score: 0.9635\n",
      "[tuning] XGBoost: Best params: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "[tuning] XGBoost: Refitting best estimator on the full training set ...\n",
      "[tuning] XGBoost: Refit finished in 0.13 min.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost tuning (if available)\n",
    "if 'XGBoost' in param_distributions:\n",
    "    xgb_est = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "    xgb_param_dist = param_distributions['XGBoost']\n",
    "    best_xgb, best_xgb_score = tune_and_refit('XGBoost', xgb_est, xgb_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['XGBoost'] = best_xgb\n",
    "    tuned_scores['XGBoost'] = best_xgb_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9e8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Logistic Regression: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[tuning] Logistic Regression: RandomizedSearchCV finished in 21.08 min. Best CV score: 0.8135\n",
      "[tuning] Logistic Regression: Best params: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None, 'C': 0.1}\n",
      "[tuning] Logistic Regression: Refitting best estimator on the full training set ...\n",
      "[tuning] Logistic Regression: Refit finished in 0.05 min.\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression tuning\n",
    "if 'Logistic Regression' in param_distributions:\n",
    "    lr_est = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_param_dist = param_distributions['Logistic Regression']\n",
    "    best_lr, best_lr_score = tune_and_refit('Logistic Regression', lr_est, lr_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['Logistic Regression'] = best_lr\n",
    "    tuned_scores['Logistic Regression'] = best_lr_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Neural Network: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# Neural Network tuning (MLP)\n",
    "if 'Neural Network' in param_distributions:\n",
    "    nn_est = MLPClassifier(random_state=42, max_iter=500)\n",
    "    nn_param_dist = param_distributions['Neural Network']\n",
    "    best_nn, best_nn_score = tune_and_refit('Neural Network', nn_est, nn_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['Neural Network'] = best_nn\n",
    "    tuned_scores['Neural Network'] = best_nn_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any remaining models not tuned, fit defaults on the full training set\n",
    "for name, est in base_estimators.items():\n",
    "    if name in trained_models:\n",
    "        continue\n",
    "    if est is None:\n",
    "        continue\n",
    "    print(f\"[fit-default] Fitting default estimator for {name} on full training set ...\")\n",
    "    t0 = time.time()\n",
    "    est.fit(features_train, target_train)\n",
    "    t1 = time.time()\n",
    "    print(f\"[fit-default] {name} fitted in {(t1-t0)/60:.2f} min.\")\n",
    "    trained_models[name] = est\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpp (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
