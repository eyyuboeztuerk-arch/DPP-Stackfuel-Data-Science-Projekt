{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90de5850",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 03_modeling.ipynb\n",
    "# Modeling and Evaluation for Diabetes Binary Classification\n",
    "# Extended with Hyperparameter Tuning, Multiple Models, ANN, and Cost-Benefit Analysis\n",
    "# Integrated with preprocessing module\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Notes:\n",
    "- RandomizedSearchCV is used for efficiency.\n",
    "- For expensive models (RF, XGBoost) tuning runs on a 30% subsample, then the best estimator is refit on the full training set.\n",
    "- CV for RandomizedSearchCV is set to cv=5 as requested.\n",
    "- MLP tuning uses early stopping and reduced max_iter for speed.\n",
    "- Best models are saved to disk for use in 04_results.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from pyexpat import features\n",
    "\n",
    "# Add src directory to Python path to import preprocessing module\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import preprocessing module\n",
    "try:\n",
    "    from dpp.preprocessing import get_preprocessed_data\n",
    "    PREPROCESSING_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    PREPROCESSING_AVAILABLE = False\n",
    "    print(f\"Warning: Could not import preprocessing module. Error: {e}\")\n",
    "    print(\"Using placeholder data for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1eb0c6",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0387f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD PREPROCESSED DATA\n",
      "================================================================================\n",
      "[preprocessing] Loading data from: C:\\Users\\Eyyub\\Desktop\\StackFuel\\PortfolioProjekt\\DPP-Stackfuel-Data-Science-Projekt\\data\\raw\\diabetes-health-indicators-dataset\\diabetes_binary_health_indicators_BRFSS2015.csv\n",
      "Training set size: 311,002\n",
      "Test set size: 45,895\n",
      "Number of features: 24\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD PREPROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOAD PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if PREPROCESSING_AVAILABLE:\n",
    "    # Load preprocessed data from preprocessing module\n",
    "    try:\n",
    "        data = get_preprocessed_data()\n",
    "        features_train = data['features_train']\n",
    "        target_train = data['target_train']\n",
    "        features_test = data['features_test']\n",
    "        target_test = data['target_test']\n",
    "\n",
    "        print(f\"Training set size: {len(features_train):,}\")\n",
    "        print(f\"Test set size: {len(features_test):,}\")\n",
    "        print(f\"Number of features: {features_train.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Using placeholder data for demonstration.\")\n",
    "        PREPROCESSING_AVAILABLE = False\n",
    "else:\n",
    "    print(\"Preprocessing module not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198bf6c",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7070d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL INITIALIZATION (BASE ESTIMATORS)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. MODEL INITIALIZATION (base estimators)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL INITIALIZATION (BASE ESTIMATORS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "base_estimators = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    # Use LinearSVC as base; wrap in CalibratedClassifierCV later if you need probabilities\n",
    "    'SVM': CalibratedClassifierCV(LinearSVC(random_state=42, max_iter=10000, dual=False), cv=5),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=500)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7521fc",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eb4f8",
   "metadata": {},
   "source": [
    "In this section, hyperparameter tuning is performed only for four selected models:\n",
    "Random Forest, XGBoost, Logistic Regression, and Neural Network.\n",
    "\n",
    "The rationale behind this selective tuning approach is based on a balance between \n",
    "computational efficiency and expected impact on model performance:\n",
    "\n",
    "1. Effectiveness of Hyperparameters:\n",
    "   These four models have hyperparameters that significantly influence their \n",
    "   predictive performance. For example, the number of trees and depth in Random Forest,\n",
    "   learning rate and tree depth in XGBoost, regularization strength in Logistic Regression,\n",
    "   and architecture parameters in Neural Networks can greatly affect results.\n",
    "\n",
    "2. Computational Cost:\n",
    "   Exhaustive hyperparameter tuning (e.g., GridSearchCV) is computationally expensive,\n",
    "   especially when applied to many models with large parameter grids. Focusing on \n",
    "   models with the highest potential gain optimizes resource usage.\n",
    "\n",
    "3. Simplicity and Speed of Other Models:\n",
    "   Other models like Decision Trees, K-Nearest Neighbors, Naive Bayes, and Linear SVM \n",
    "   typically have fewer or less impactful hyperparameters, or are inherently faster to train.\n",
    "   Their default parameters often provide reasonable baseline performance.\n",
    "\n",
    "4. Practical Workflow:\n",
    "   This approach allows for a manageable and efficient modeling pipeline, prioritizing \n",
    "   tuning efforts where they are most likely to yield substantial improvements.\n",
    "\n",
    "If desired, hyperparameter tuning can be extended to additional models using more \n",
    "efficient search strategies (e.g., RandomizedSearchCV) or by tuning a smaller subset \n",
    "of parameters to balance performance gains with computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c45eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "Note: Tuning for expensive models (RF, XGB) runs on a 30% subsample to save time.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. HYPERPARAMETER TUNING (RandomizedSearchCV, CV=5)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"Note: Tuning for expensive models (RF, XGB) runs on a 30% subsample to save time.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a 30% subsample for expensive model tuning\n",
    "features_sub, _, target_sub, _ = train_test_split(features_train, target_train, train_size=0.30, stratify=target_train, random_state=42)\n",
    "\n",
    "# Parameter distributions for RandomizedSearch\n",
    "param_distributions = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [8, 12, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', 0.5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "# We will store the final trained models here\n",
    "trained_models = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf68775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run RandomizedSearchCV and refit best estimator on full training set\n",
    "def tune_and_refit(name, estimator, param_dist, X_tune, y_tune, X_full, y_full, n_iter=20, cv=5):\n",
    "    \"\"\"\n",
    "    Tune estimator with RandomizedSearchCV on X_tune/y_tune (subsample for speed),\n",
    "    then refit the best estimator on the full training data X_full/y_full.\n",
    "    Returns the refitted best estimator and the best CV score.\n",
    "    \"\"\"\n",
    "    print(f\"[tuning] {name}: Starting RandomizedSearchCV (n_iter={n_iter}, cv={cv}) ...\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring='roc_auc',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rs.fit(X_tune, y_tune)\n",
    "    t1 = time.time()\n",
    "    print(f\"[tuning] {name}: RandomizedSearchCV finished in {(t1-t0)/60:.2f} min. Best CV score: {rs.best_score_:.4f}\")\n",
    "    print(f\"[tuning] {name}: Best params: {rs.best_params_}\")\n",
    "\n",
    "    # Refit best estimator on full training data for final model\n",
    "    best_est = rs.best_estimator_\n",
    "    print(f\"[tuning] {name}: Refitting best estimator on the full training set ...\")\n",
    "    t2 = time.time()\n",
    "    best_est.fit(X_full, y_full) # pyright: ignore[reportAttributeAccessIssue]\n",
    "    t3 = time.time()\n",
    "    print(f\"[tuning] {name}: Refit finished in {(t3-t2)/60:.2f} min.\")\n",
    "    return best_est, rs.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2434bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Random Forest: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[tuning] Random Forest: RandomizedSearchCV finished in 14.17 min. Best CV score: 0.9598\n",
      "[tuning] Random Forest: Best params: {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': None}\n",
      "[tuning] Random Forest: Refitting best estimator on the full training set ...\n",
      "[tuning] Random Forest: Refit finished in 0.59 min.\n"
     ]
    }
   ],
   "source": [
    "# Tuning loop for selected models\n",
    "tuned_models = {}\n",
    "tuned_scores = {}\n",
    "\n",
    "# Random Forest tuning (on subsample, then refit on full)\n",
    "if 'Random Forest' in base_estimators:\n",
    "    rf_est = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    rf_param_dist = param_distributions['Random Forest']\n",
    "    best_rf, best_rf_score = tune_and_refit('Random Forest', rf_est, rf_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['Random Forest'] = best_rf\n",
    "    tuned_scores['Random Forest'] = best_rf_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94e7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] XGBoost: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[tuning] XGBoost: RandomizedSearchCV finished in 2.18 min. Best CV score: 0.9635\n",
      "[tuning] XGBoost: Best params: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "[tuning] XGBoost: Refitting best estimator on the full training set ...\n",
      "[tuning] XGBoost: Refit finished in 0.16 min.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost tuning (if available)\n",
    "if 'XGBoost' in param_distributions:\n",
    "    xgb_est = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "    xgb_param_dist = param_distributions['XGBoost']\n",
    "    best_xgb, best_xgb_score = tune_and_refit('XGBoost', xgb_est, xgb_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['XGBoost'] = best_xgb\n",
    "    tuned_scores['XGBoost'] = best_xgb_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9e8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Logistic Regression: Starting RandomizedSearchCV (n_iter=20, cv=5) ...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[tuning] Logistic Regression: RandomizedSearchCV finished in 23.56 min. Best CV score: 0.8135\n",
      "[tuning] Logistic Regression: Best params: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None, 'C': 0.1}\n",
      "[tuning] Logistic Regression: Refitting best estimator on the full training set ...\n",
      "[tuning] Logistic Regression: Refit finished in 0.05 min.\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression tuning\n",
    "if 'Logistic Regression' in param_distributions:\n",
    "    lr_est = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_param_dist = param_distributions['Logistic Regression']\n",
    "    best_lr, best_lr_score = tune_and_refit('Logistic Regression', lr_est, lr_param_dist, features_sub, target_sub, features_train, target_train, n_iter=20, cv=5)\n",
    "    trained_models['Logistic Regression'] = best_lr\n",
    "    tuned_scores['Logistic Regression'] = best_lr_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a9e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tuning] Neural Network: Starting RandomizedSearchCV (n_iter=10, cv=5) ...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[tuning] Neural Network: RandomizedSearchCV finished in 17.54 min. Best CV score: 0.9196\n",
      "[tuning] Neural Network: Best params: {'learning_rate': 'constant', 'hidden_layer_sizes': (100,), 'alpha': 0.001, 'activation': 'relu'}\n",
      "[tuning] Neural Network: Refitting best estimator on the full training set ...\n",
      "[tuning] Neural Network: Refit finished in 1.96 min.\n"
     ]
    }
   ],
   "source": [
    "# --- Neural Network tuning (MLP), FASTER VERSION ---\n",
    "# Use a 30% subsample for tuning, enable early stopping, reduce max_iter, and reduce n_iter\n",
    "if 'Neural Network' in param_distributions:\n",
    "    # Subsample for MLP tuning\n",
    "    X_sub_nn, _, y_sub_nn, _ = train_test_split(features_train, target_train, train_size=0.30, stratify=target_train, random_state=42)\n",
    "\n",
    "    # Base estimator with early stopping and reduced iterations for tuning\n",
    "    nn_est = MLPClassifier(random_state=42, max_iter=200, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "    nn_param_dist = param_distributions['Neural Network']\n",
    "\n",
    "    # Randomized search with fewer iterations (n_iter=10) and cv=5\n",
    "    best_nn, best_nn_score = tune_and_refit(\n",
    "        'Neural Network',\n",
    "        nn_est,\n",
    "        nn_param_dist,\n",
    "        X_sub_nn,\n",
    "        y_sub_nn,\n",
    "        features_train,\n",
    "        target_train,\n",
    "        n_iter=10,   # smaller search budget for speed\n",
    "        cv=5\n",
    "    )\n",
    "    trained_models['Neural Network'] = best_nn\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05eb2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit-default] Fitting default estimator for SVM on full training set ...\n",
      "[fit-default] SVM fitted in 0.23 min.\n",
      "[fit-default] Fitting default estimator for K-Nearest Neighbors on full training set ...\n",
      "[fit-default] K-Nearest Neighbors fitted in 0.00 min.\n",
      "[fit-default] Fitting default estimator for Decision Tree on full training set ...\n",
      "[fit-default] Decision Tree fitted in 0.08 min.\n",
      "[fit-default] Fitting default estimator for Naive Bayes on full training set ...\n",
      "[fit-default] Naive Bayes fitted in 0.00 min.\n"
     ]
    }
   ],
   "source": [
    "# For any remaining models not tuned, fit defaults on the full training set\n",
    "for name, est in base_estimators.items():\n",
    "    if name in trained_models:\n",
    "        continue\n",
    "    if est is None:\n",
    "        continue\n",
    "    print(f\"[fit-default] Fitting default estimator for {name} on full training set ...\")\n",
    "    t0 = time.time()\n",
    "    est.fit(features_train, target_train)\n",
    "    t1 = time.time()\n",
    "    print(f\"[fit-default] {name} fitted in {(t1-t0)/60:.2f} min.\")\n",
    "    trained_models[name] = est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6728b",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b897c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Evaluating Random Forest ...\n",
      "AUC Score: 0.7887\n",
      "CV AUC Score: 0.9751 (+/- 0.0906)\n",
      "\n",
      "Evaluating XGBoost ...\n",
      "AUC Score: 0.8153\n",
      "CV AUC Score: 0.9511 (+/- 0.1847)\n",
      "\n",
      "Evaluating Logistic Regression ...\n",
      "AUC Score: 0.8078\n",
      "CV AUC Score: 0.8155 (+/- 0.0115)\n",
      "\n",
      "Evaluating Neural Network ...\n",
      "AUC Score: 0.7897\n",
      "CV AUC Score: 0.9234 (+/- 0.1606)\n",
      "\n",
      "Evaluating SVM ...\n",
      "AUC Score: 0.8075\n",
      "CV AUC Score: 0.8152 (+/- 0.0116)\n",
      "\n",
      "Evaluating K-Nearest Neighbors ...\n",
      "AUC Score: 0.7070\n",
      "CV AUC Score: 0.9198 (+/- 0.0120)\n",
      "\n",
      "Evaluating Decision Tree ...\n",
      "AUC Score: 0.5901\n",
      "CV AUC Score: 0.8469 (+/- 0.2513)\n",
      "\n",
      "Evaluating Naive Bayes ...\n",
      "AUC Score: 0.7647\n",
      "CV AUC Score: 0.7708 (+/- 0.0120)\n",
      "\n",
      "Best model: XGBoost with AUC: 0.8153\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. MODEL EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_results = {}\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\nEvaluating {name} ...\")\n",
    "    # Ensure model supports predict_proba; if not, try to obtain calibrated probabilities\n",
    "    try:\n",
    "        y_proba = model.predict_proba(features_test)[:, 1]\n",
    "    except Exception:\n",
    "        # fallback: use decision_function and min-max scale (heuristic) - ideally calibrate with holdout\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            scores = model.decision_function(features_test)\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            y_proba = MinMaxScaler().fit_transform(scores.reshape(-1, 1)).ravel()\n",
    "        else:\n",
    "            # last-resort: use predict (not ideal)\n",
    "            y_proba = model.predict(features_test)\n",
    "\n",
    "    y_pred = (y_proba >= 0.5).astype(int) if y_proba.ndim == 1 else model.predict(features_test)\n",
    "\n",
    "    auc_score = roc_auc_score(target_test, y_proba)\n",
    "    cv_scores = cross_val_score(model, features_train, target_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"CV AUC Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['auc'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_y_proba = model_results[best_model_name]['y_proba']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with AUC: {model_results[best_model_name]['auc']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpp (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
